{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c033ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "from scipy import stats\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1fd48",
   "metadata": {},
   "source": [
    "### Target Encoding with smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ca855",
   "metadata": {},
   "source": [
    "`min_samples_leaf` は、あるカテゴリ値に対して、事前確率（全体平均）とそのカテゴリのターゲット平均が同じ重みを持つようになるしきい値を定義します。\n",
    "\n",
    "このしきい値よりも下では事前確率の方が重要になり、上ではカテゴリごとの平均値の方が重要になります。\n",
    "\n",
    "カテゴリの出現回数に対する重みの変化の仕方は、`smoothing` パラメータによって制御されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe8d4a8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "カテゴリごとの出現回数 $\\text{count}$ を使って、スムージング係数 $\\text{smoothing}$ を次のように計算します。\n",
    "\n",
    "$$\n",
    "\\text{smoothing} = \\frac{1}{1 + \\exp\\left(-\\frac{\\text{count} - \\text{min\\_samples\\_leaf}}{\\text{smoothing}}\\right)}\n",
    "$$\n",
    "\n",
    "- $\\text{count}$ ：カテゴリの出現回数\n",
    "- $\\text{min\\_samples\\_leaf}$：最小サンプル数（小さいカテゴリに対して過信しすぎないための基準）\n",
    "- $\\text{smoothing}$（引数）：スムージングの強さを調整するパラメータ\n",
    "\n",
    "この式は、ロジスティック関数（シグモイド関数）に似た形をしており、  \n",
    "カテゴリの出現回数が増えるとスムージング係数が 1 に近づき、  \n",
    "出現回数が少ないとスムージング係数が 0 に近づきます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68456833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(series,noise_level=0.01):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e3f82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode(trn_series=None, \n",
    "                  tst_series=None, \n",
    "                  target=None, \n",
    "                  min_samples_leaf=1, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    trn_series:訓練用のカテゴリ変数(pandas Series)\n",
    "    tst_series:テスト用のカテゴリ変数(pandas Series)\n",
    "    target:目的変数(pandas Series)\n",
    "    min_samples_leaf:小さいカテゴリの平均を信用しすぎないためのパラメータ\n",
    "    smoothing:スムージング強度（滑らかにする）\n",
    "    noise_level:後で少しノイズを加える割合（オーバーフィット防止）\n",
    "    \"\"\" \n",
    "    # 訓練データとターゲットの長さが一致しているか確認\n",
    "    assert len(trn_series) == len(target)\n",
    "    # 訓練とテストでカラム名が一致しているか確認\n",
    "    assert trn_series.name == tst_series.name\n",
    "    \n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # priorは全体のターゲット平均。それと各カテゴリの平均をスムージングで混ぜる。\n",
    "    # それと各カテゴリの平均をスムージングで混ぜる。\n",
    "    #「東京の生存率はサンプル数少ないから、全体平均もちょっと混ぜるね」みたいな調整。\n",
    "    prior = target.mean()\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    # 最終的に使うのはスムージングされた平均だけ。\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "\n",
    "    ft_trn_series = pd.merge(\n",
    "        # SeriesをDataFrameに変換\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "\n",
    "    ft_trn_series.index = trn_series.index \n",
    "\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "\n",
    "    ft_tst_series.index = tst_series.index\n",
    "\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76415d25",
   "metadata": {},
   "source": [
    "### Testing with ps_car_11_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f80f710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "7     0.038848\n",
       "9     0.023906\n",
       "13    0.031488\n",
       "16    0.044360\n",
       "17    0.026202\n",
       "19    0.045251\n",
       "20    0.022511\n",
       "22    0.030943\n",
       "26    0.034956\n",
       "28    0.044711\n",
       "Name: ps_car_11_cat_mean, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading data\n",
    "trn_df = pd.read_csv(\"data/train.csv\", index_col=0)\n",
    "sub_df = pd.read_csv(\"data/test.csv\", index_col=0)\n",
    "\n",
    "# Target encode ps_car_11_cat\n",
    "trn, sub = target_encode(trn_df[\"ps_car_11_cat\"], \n",
    "                         sub_df[\"ps_car_11_cat\"], \n",
    "                         target=trn_df.target, \n",
    "                         min_samples_leaf=100,\n",
    "                         smoothing=10,\n",
    "                         noise_level=0.01)\n",
    "trn.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a39c7",
   "metadata": {},
   "source": [
    "### Scatter plot of category values vs taret encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905d8c6",
   "metadata": {},
   "source": [
    "### Check AUC metric improvement after noisy encoding over 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c85e541c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Raw Categories |   Encoded Categories\n",
      "ps_ind_02_cat        :  0.506204 + 0.003512 | 0.510449 + 0.004402\n",
      "ps_ind_04_cat        :  0.512617 + 0.003800 | 0.513788 + 0.006141\n",
      "ps_ind_05_cat        :  0.520249 + 0.003626 | 0.534299 + 0.003785\n",
      "ps_car_01_cat        :  0.528913 + 0.003454 | 0.551347 + 0.003333\n",
      "ps_car_02_cat        :  0.531614 + 0.002454 | 0.529862 + 0.002166\n",
      "ps_car_03_cat        :  0.539650 + 0.002949 | 0.540559 + 0.004684\n",
      "ps_car_04_cat        :  0.536473 + 0.001356 | 0.536138 + 0.002215\n",
      "ps_car_05_cat        :  0.530585 + 0.005116 | 0.530724 + 0.006458\n",
      "ps_car_06_cat        :  0.515693 + 0.002867 | 0.541883 + 0.004697\n",
      "ps_car_07_cat        :  0.522624 + 0.000995 | 0.522380 + 0.002023\n",
      "ps_car_08_cat        :  0.520287 + 0.003198 | 0.517903 + 0.005479\n",
      "ps_car_09_cat        :  0.504890 + 0.003429 | 0.524150 + 0.003633\n",
      "ps_car_10_cat        :  0.500385 + 0.000405 | 0.498746 + 0.003641\n",
      "ps_car_11_cat        :  0.512416 + 0.004256 | 0.568516 + 0.003962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# StratifiedKFoldで分割設定\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# '_cat'を含むカテゴリ特徴量だけリストアップ\n",
    "f_cats = [f for f in trn_df.columns if \"_cat\" in f]\n",
    "\n",
    "# ヘッダー出力\n",
    "print(\"%20s   %20s | %20s\" % (\"\", \"Raw Categories\", \"Encoded Categories\"))\n",
    "\n",
    "# 各カテゴリ特徴量に対して\n",
    "for f in f_cats:\n",
    "    print(\"%-20s : \" % f, end=\"\")\n",
    "    \n",
    "    e_scores = []  # エンコード後スコア格納\n",
    "    f_scores = []  # エンコード前スコア格納\n",
    "    \n",
    "    # 5-fold cross validation\n",
    "    for trn_idx, val_idx in folds.split(trn_df.values, trn_df.target.values):\n",
    "        \n",
    "        # ❗ここを正しく修正（val_idxを使う！）\n",
    "        trn_f, trn_tgt = trn_df[f].iloc[trn_idx], trn_df.target.iloc[trn_idx]\n",
    "        val_f, val_tgt = trn_df[f].iloc[val_idx], trn_df.target.iloc[val_idx]\n",
    "\n",
    "        # target encodingを学習用と検証用に適用\n",
    "        trn_tf, val_tf = target_encode(\n",
    "            trn_series=trn_f,\n",
    "            tst_series=val_f,\n",
    "            target=trn_tgt,\n",
    "            min_samples_leaf=100,\n",
    "            smoothing=20,\n",
    "            noise_level=0.01\n",
    "        )\n",
    "\n",
    "        # 生データのROC-AUC（値が小さい場合は1-AUCを取る）\n",
    "        raw_auc = roc_auc_score(val_tgt, val_f)\n",
    "        f_scores.append(max(raw_auc, 1 - raw_auc))\n",
    "        \n",
    "        # エンコード後データのROC-AUC\n",
    "        e_scores.append(roc_auc_score(val_tgt, val_tf))\n",
    "    \n",
    "    # Fold平均と標準偏差を出力\n",
    "    print(\" %.6f + %.6f | %6f + %.6f\" \n",
    "          % (np.mean(f_scores), np.std(f_scores), np.mean(e_scores), np.std(e_scores)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
